---
title : 이미지 분류 모델 with 심층 신경망
toc : true
toc_sticky : true
---
> *"part 3 케라스(Keras) 03 심층 신경망으로 이미지 분류"에 대한 정리*


## 3.1 케라스 내장 데이터셋 로드
> 케라스 자체적으로 모델 훈련에 쓸 수 있는 샘플 데이터셋을 제공  
> 패키지 위치 : tensorflow.keras.datasets

<br> 1. 케라스 dataset 불러오기(mnist)
```python
import tensorflow as tf

#케라스 내장 데이터셋에서 mnits 데이터셋 로드 + mnist 변수에 대입
mnist = tf.keras.datasets.mnist
```

<br> 2. mnist 변수에 load_data() 메소드 적용  
메소드 적용시 : 넘파이 배열로 구성된 데이터셋이 다운로드 된다.  
다운로드 내용 : 훈련 데이터셋 + 검증 데이터셋  
투플 형태로 데이터셋을 저장한다  
```python
#load_data() : 데이터셋 로드
(x_train, y_train),(x_test, y_test) = mnist.load_data()
```
<br> 3. 결과 출력시 : 훈련 셋 (60,000개) + 검증 셋 (10,000개) 데이터의 양 확인 가능
```python
# 데이터셋 확인
print('train set: ', x_train.shape, y_train.shape)
print('test  set: ', x_test.shape, y_test.shape)
```
출력 결과의 의미 : (1) 28 X 28 픽셀 크기의 정사각형 이미지 각 60,000장, 10,000장이 저장되어 있다. (2) 각 데이터가 1차원 벡터 형태로 정리되어 있다.

<br> 4. 데이터 이미지 시각화해보기  
matplotlob 라이브러리를 이용한다.

```python
import matplotlib.pyplot as plt

fig.axes = plt.subplots(3, 5)
fig.set_size_inches(8, 5)

for i in range(15):
    ax = axes[i//5, i%5]
    # 이미지 시각화 : imshow
    ax.imshow(x_train[i], cmap='gray')
    ax.axis('off')
    ax.set_title(Str(y_train[i]))

plt.tight_layout()
plt.show()
```

## 3.2 데이터 전처리
**데이터 전처리에서 하는 일** : 정규화를 이용해 데이터 범위를 조절  
**전처리 전 x_train** : 각 데이터의 원소는 0~255 범위의 값을 가짐 ( 픽셀 값과 일대일 매칭 )  
**전처리 후 x_train** : 각 데이터의 원소가 0~1 범위의 값을 가지도록 조정  
**어떻게 전처리?** 데이터 최대값인 255로 나눠줌

```python
print(f'정규화 전] 최소값: {x_train.min()}, 최대값: {x_train.max()}')

#훈련셋, 검증셋 데이터 정규화
x_train = x_train / x_train.max()
x_test = x_test / x_test.max()

print(f'정규화 후] 최소값: {x_train.min()}, 최대값: {x_train.max()}')

#변환 후 x_train 배열의 데이터 확인
x_train[0, 10:15, 10:15]
```

> **왜 정규화를 해야 할까?**  
> 
> 수렴 속도 : 정규화된 데이터 >>>> 비정규화된 데이터  
> 국소 최적 현상(local optimum) 방지 효과

## 3.3 Flatten 레이어
데이터를 학습시키기 전 알아야 하는 point : 데이터의 형태  
1. 샘플 이미지의 형태 : (28, 28) ← 2차원 입력
2. 2차원 입력은 Dense 레이어 사용 불가능<br>
   Dense 레이어의 입력값 : **1차원 배열**만 가능!  
   Dense 레이어를 사용하려면 1차원 전처리 과정이 필요!
- 해결 방법 1) 이미지 데이터를 1차원으로 전처리하기
- 해결 방법 2) Dense 레이어 말고 Flatten 레이어를 사용하기
  - flatten 레이어 : 다차원 데이터를 1차원으로 펼쳐주기

```python
# 해결 방법 1)
print(f'변경 전 shape: {x_train.shape}')
print(f'1D으로 shape 변경: {x_train.reshape(60000, -1).shape}')
```

```python
# 해결 방법 2)
print(f'변경 전 shape: {x_train.shape}')
print(f'Flatten 적용 후: {tf.keras.layers.Flatten()(x_train).shape}')
```
적용 결과: shape이 (60000, 784)로 출력된다 (784 = 28 *28)

## 3.4 활성화 함수(activation)
**활성화 함수** : 입력을 비선형 출력으로 변환해주는 함수  
→ 즉, 선형관계 함수에 비선형성을 추가하는 것

**비선형성을 추가하는 이유** : 선형 함수로만 층을 구성하는 것의 한계를 보완  
**only 선형함수의 한계** : 모델을 깊게 구성하더라도 선형함수로밖에 표현이 안된다

**자주 쓰이는 활성화 함수** : Sigmoid, Hyperbolic Tangent, ReLU, Leaky ReLu 등

코드에 활성화 함수를 적용해보자  
활성화 함수 관련 매개변수 : activation  
`tf.keras.layers.Dense(128, activation='relu')`

응용) 레이어 구성 커스터마이징  
레이어에 활성화 함수 적용시 별도의 층처럼 적용 가능하다.  

별도로 적용하는 이유?  
Dense 레이어 다음에 배치 정규화(Batch Normalization)를 적용할 수 활성화 함수를 적용하기 위해서.
```python
model = tf.keras.Sequential([
  tf.keras.laters.Dense(128),
  tf.keras.layers.Activation('relu')
])
```
> 배치 정규화는 다음 게시물에서 다룬다.

## 3.5 입력과 출력
**- 입력**

- 분류 모델의 가장 첫번째 레이어  
- 첫 번째 레이어에 input_shape 매개변수 지정은 일반적인 step  
input_shape 매개변수 : 입력 데이터의 형태를 나타냄

```python
model = tf.keras.Sequential([
  tf.keras.layers.Flatten(input_shape=(28,28)),
  tf.keras.layers.Dense(256, activation='relu'),
  tf.keras.layers.Dense(64, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])
```
**- 출력**

- 분류 모델의 가장 마지막 레이어  
**point) 출력층 노드 개수 == 분류해야 할 클래스 개수**  
(mnist 데이터의 경우, 10개의 클래스가 존재하므로 마지막 출력층의 노드 개수는 10개인 것)

- 출력층의 노드 개수가 2개 이상이라면 : softmax 함수 적용하자  

- 활성화 함수는 무엇을 적용해야 할까?  
**- softmax 함수** : 다중 분류 문제  
**- sigmoid 함수** : 이진 분류 문제 && 출력층 노드 개수가 1개  
(이진 분류 문제여도 출력 레이어의 노드 개수가 2개라면 softmax 함수를 사용한다.)  
`tf.keras.layers.Dense(1, activation='sigmoid')`  
`tf.keras.layers.Dense(10, activation='softmax')`

## 3.6 손실함수(loss)
손실함수 : 모델의 정상적 훈련 관장
- 출력층의 노드 개수 : 1 (이진 분류)
  - activation : sigmoid
  - loss : binary_crossentropy
- 출력층의 노드 개수 : 2 이상
  - activation : softmax
  - loss : *당신의 출력 데이터가 원핫 인코딩 되었나요?*
    - YES : categorical_crossentropy
    - NO : sparse_categorical_crossentropy

```python
#이진 분류(출력 노드 1개, sigmoid 활성화 함수)
model.compile(loss='binary_crossentropy')

#y : 원핫 벡터 O
model.compile(loss='categorical_crossentropy')

#y : 원핫 벡터 X
model.compile(loss='sparse_categorical_crossentropy')
```

## 3.7 옵티마이저(optimizer)
- 옵티마이저 : 손실을 낮추기 위해 신경망의 속성(가중치, 학습률 등)을 변경하는 데 사용되는 최적화 방법  

- 일반적으로 사용되는 알고리즘 : Adam  
  
- 케라스에서 제공하는 알고리즘 : SGD / Adam / Adagrad / Nadam / RMSprop / Adadelta / Adamax / Ftrl

- 클래스 인스턴스, 문자열 형식 모두 지원  
(클래스 인스턴스로 지정 시 하이퍼파라미터를 직접 설정할 수 있다)

```python
#클래스 인스턴스 ver
adam = tf.keras.optimizers.Adam(lr=0.001)
model.compile(optimizer=adam)

#문자열 ver
model.compile(optimizer='adam')
```

## 3.8 평가지표(metrics)
가장 많이 사용되는 평가지표 : accuracy

지정 시 클래스 인스턴스 / 문자열 모두 사용 가능

```python
acc = tf.keras.metrics.SparseCategoricalAccuracy()

model.compile(optimizer='adam',
loss='sparse_categorical_crossentropy',
metics = [acc])

model.compile(optimizer='adam',
loss='sparse_categorical_crossentropy',
metics = ['accuracy'])
```
## 3.9 훈련
`model.fit(x_train, y_train,
validation_data=(x_test, y_test),
epochs=10)`

validation_data : 검증 셋 추가 지정

## 3.10 평가
evaluate() 메소드
```python
test_loss, test_acc = model.evaluate(x_test, y_test)
print('검증 셋 정확도 : ', test_acc)
```
## 3.11 예측
predict() 메소드  
~~검증 데이터셋의 입력 데이터 x_test를 재사용~~

`predictions = model.predict(x_test)` : 넘파이 배열 형태로 저장되는 예측 분류 결과

`predictions[0]` : 분류 결과 출력 시, 10개 값이 제시되며 가장 높은 확률값을 가지는 클래스가 최종 예측 클래스가 된다.

넘파이 배열의 argmax를 이용해 가장 높은 확률값의 클래스 결과를 확인할 수 있다.  
```python
import numpy as np
print(np.argmax(predictions[0]))
print(np.argmax(predictions[:10], axis=1))
```

