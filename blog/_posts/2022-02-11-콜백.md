---
title : 콜백
toc : true
toc_sticky : true
---

> “part 3 케라스(Keras) 05 콜백”에 대한 정리

콜백이란,  
모델을 훈련할 때 보조적인 작업을 수행할 수 있도록 도움을 주는 객체이다.

모델 훈련 시 활용되는 객체이므로, fit() 메소드에서 매개변수로 접근이 가능하다.  

콜백 함수에는 여러가지 종류가 있다.

앞선 포스팅에서 계속 사용하고 있는 예제를 가지고 콜백에 대한 실습을 진행하자.

<br>
예제 모델

```python
import tensor flow as tf

mnist = tf.keras.datasets.mnist # 케라스 내장 데이터셋에서 mnist 데이터셋 로드

(x_train, y_train), (x_test, y_test) = mnist.load_data() # load_data()로 데이터셋 로드

# 데이터 정규화
x_train = x_train / x_train.max()
y_train = y_train / y_train.max()

# 모델 정의
model = tf.keras.Sequential([tf.keras.laters.Flatten(input_shape=(28, 28)),
tf.keras.layers.Dense(256, activation='relu'),
tf.keras.layers.Dense(64, activation='relu'),
tf.keras.layers.Dense(32, activation='relu'),
tf.keras.layers.Dense(10, activation='relu'), # 출력 노드 10개로 설정
])

# 모델 컴파일
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
```

## 5.1 모델 체크포인트(Model Checkpoint)
콜백 중 가장 많이 활용되는 것이 모델 체크포인트이다. epoch 별 모델의 가중치를 저장한다.
- filepath  : 체크포인트의 저장 경로 지정
- save_weights_only : 가중치만 저장할지 여부 설정
- save_best_only : monitor 기준으로 가장 높은 epoch만 저장할지 / 매 epoch 별 저장할지 여부를 설정
- monitor : 저장 시 기준이 되는 지표를 설정  
  ex_’val_loss’로 지정 = 검증 손실이 가장 낮은 epoch의 가중치를 저장한다.
- verbose : 1로 저장하면 epoch별 저장 여부를 알려주는 로그메시지를 출력

<br>모델 체크포인트 객체 생성하기
```python
checkpoint = tf.keras.callbacks.ModelCheckPoint(
    filepath='tmp_checkpoint.ckpt',
    save_weights_only=True,
    save_best_only=True,
    monitor='val_loss',
    verbose=1
    ) 
``` 

위에서 생성한 체크포인트 객체를 모델 훈련 시 callbacks 매개변수에 지정하면 된다.
```python
model.fit(x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=10,
    callbacks=[checkpoint])
```

앞서 생성한 객체의 설정값에 바탕하여 모델을 훈련시키면
매 에포크마다 모델 체크포인트의 저장 여부를 알려주는 로그 메시지가 출력되고,
가장 검증 손실이 낮았던 에포크의 가중치가 체크포인트에 저장되는 결과를 얻을 수 있다.

저장된 가중치를 모델 인스턴스에 적용할 수 있다. load_weights() 메소드에 모델 체크포인트 파일 경로를 지정하여 호출하면 된다.
모델에 저장한 가중치를 명시적으로 로드하면 검증 손실이 가장 낮았던 가중치가 모델에 로드되는 것이다.
```python
# 모델 체크포인트 로드 전
loss, acc = model.evaluate(x_test, y_test)
print(f'체크포인트 로드 전: loss: {loss:3f}, acc: {acc:3f}')

# 체크포인트 파일을 모델에 로드
model.load_weights('tmp_checkpoint.ckpt') # 명시적으로 로드해주어야만 원하는 가중치를 로드시킬 수 있다.
loss, acc = model.evaluate(x_test, y_test) 
print(f'체크포인트 로드 후: loss: {loss:3f}, acc: {acc:3f}')
```

## 5.2 조기 종료(Early Stopping)
말 그대로 모델의 훈련을 일찍 멈추는 기능을 수행한다.  
모델이 조기 종료되기 위한 조건은 손실값에 달려있다. 모델 훈련 시 patience에 지정된 epoch 안에 손실이 줄어들지 않는 경우 훈련이 종료되는 것이다.  
tensorflow.keras.callbacks.EarlyStopping() 객체로 생성한다.

예제 코드에서 조기 종료를 설정해보자.  
먼저 조기 종료 콜백을 생성한다.  
조기 종료 기준이 되는 지표 : 검증 손실  
patience 값 : 3  
어떤 의미냐 하면, 3 epoch 동안 손실이 줄어들지 않으면 모델 훈련을 조기 종료 하겠다는 의미다.

```python
model = tf.keras.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)),
tf.keras.layers.Dense(256, activation='relu'),
tf.keras.layers.Dense(64, activation='relu'),
tf.keras.layers.Dense(32, activation='relu'),
tf.keras.layers.Dense(10, activation='softmax'),
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# EarlyStopping 콜백 생성
earlystopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

# 모델 훈련 fit() 메소드 안에 callback 매개변수로 EarlyStopping을 지정한다
model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, callbacks=[earlystopping])
```

## 5.3 학습률 스케줄러(Learning Rate Scheduler)
학습률 스케줄러는 훈련에 대한 학습률을 조정한다.  
tensorflow.keras.callbacks.LearningRateScheduler() 객체로 생성한다.  
학습률을 특정한 로직에 의하여 제어하고자 할 때 로직을 함수로 구현한 뒤 LearningRateScheduler 객체에 적용한다.

예시 코드  
설정 : 첫 5 epoch 동안은 학습률을 유지하고, 6 epoch에서부터는 학습률을 점차 감소시킨다.
```python
def scheduler(epoch, lr):
    tf.print(f'learning_rate: {lr:.5f}')
    # 첫 5 epoch 동안 유지
    if epoch < 5 :
        return lr
    else :
        return lr * tf.math.exp(-0.1)

# 콜백 객체 생성 및 scheduler 함수 적용
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler) 
```

모델 인스턴스를 다시 생성하고 컴파일한 뒤, 초기 학습률을 출력하여 결과를 확인한다.
```python
model = tf.keras.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)),
tf.keras.layers.Dense(256, activation='relu'),
tf.keras.layers.Dense(64, activation='relu'),
tf.keras.layers.Dense(32, activation='relu'),
tf.keras.layers.Dense(10, activation='softmax'),
])

model.compile(tf.keras.optimizers.SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
print(round(model.optimizer.lr.numpy(), 5))
```

```python
# 학습률 스케줄러 적용
model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, callbacks=[lr_scheduler])

# 최종 학습률 스케쥴러 확인
round(model.optimizer.lr.numpy(), 5)
```

## 5.4 텐서보드(Tensorboard)
텐서보드 : 훈련에 대한 시각화를 실시간으로 제공하는 도구  
텐서보드는 모델 훈련 시 우리가 알아야 할 정보들을 좀 더 쉽고 빠르게 이해할 수 있도록 보여주는데 집중한다.  
모델 훈련 과정에서 시각화 차트를 실시간으로 업데이트하여 제공하는 기능도 있다.  


epoch별 훈련 손실 및 평가 지표 시각화
- 모델 구조 시각화
- 레이어 가중치 분포도 시각화  
- 등등

텐서보드를 사용하면 모두 확인해볼 수 있다!  
tensorflow.keras.callbacks.TensorBoard() 객체로 생성하여 콜백 매개변수에 적용한다.

예제 코드
```python
model = tf.keras.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)),
tf.keras.layers.Dense(256, activation='relu'),
tf.keras.layers.Dense(64, activation='relu'),
tf.keras.layers.Dense(32, activation='relu'),
tf.keras.layers.Dense(10, activation='softmax'),
])

model.compile(tf.keras.optimizers.SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# 텐서보드 저장 경로 지정
log_dir = 'tensorboard'

# 텐서보드 콜백 정의
tensorboard = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)

model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, callbacks=[tensorboard],)

# 훈련이 완료된 후 텐서보드를 출력해보자
# 노트북의 매직 커멘드를 입력하면 텐서보드를 바로 출력할 수 있다.
```python
%load_ext tensorboard
%tensorboard --logdir {log_dir}
```

