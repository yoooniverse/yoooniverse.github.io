---
title : CNN(합성곱 신경망)
toc : true
toc_sticky : true
---

> "part 4 합성곱 신경망(CNN)"에 대한 정리  
> 교재 코드는 직접 따라 적어서 [여기](https://github.com/yoooniverse/tf-practice)에 모아뒀다.


## 4.1 CNN의 간단한 개념
CNN : 합성곱 신경망 (Convolutional Neural Network)  
데이터가 가진 특징들의 패턴을 학습하는 알고리즘  
Image Classification, Object Detection, Style Transfer 분야에서 많이 사용된다.

* 합성곱 : 정방형 크기 커널을 사용해 입력 이미지에 대한 특성을 추출하는 방법
  * 합성곱으로 얻어진 특성맵, feature map을 DL 모델 학습 시 사용하면 좋은 성능을 얻을 수 있다.
  * 합성곱 연산 : "입력 이미지 X 커널"의 겹치는 부분에 대한 곱
  * 합성곱 연산을 통해 얻은 가중치(weight)  
    * 커널의 공유 가중치는 오차 역전파 시 가중치를 이용해 커널의 그래디언트를 계산할 때 연산량이 줄어든다는 장점을 가진다.
<br>
<br>
* 채널 : 입력 이미지를 구성하는 2차원 배열의 개수  
  * ex) 컬러 이미지 : R, G, B 3개의 채널을 가지는 이미지
  * 채널이 여러개일 때 합성곱 연산을 수행하는 방법
    * 채널 개수만큼 생성된 커널 → 각 채널마다 하나의 커널을 적용하여 합성곱 연산 수행<br>
    → 채널 개수만큼 만들어진 합성곱 연산 결과 → 모두 더해주면 최종 특성맵이 생성됨

ex)  
-합성곱 레이어의 출력 개수 : 20개 / 입력 이미지의 채널 개수 : 3개 / 출력 필터 : 20개
 ➜ 총 60개의 커널 생성, 커널의 사이즈 : 3 * 3  
-해당 레이어에서 업그레이드 해야 할 가중치의 그래디언트 : 3 * 3 * 입력 채널 수 * 출력 필터 수 → 3 * 3 * 3 * 20 = 540개
업그레이드 될 파라미터의 개수 : 540개 + bias 20 (출력 필터 개수만큼)

* 스트라이드(Stride)
  * 커널이 합성곱 연산을 수행할 때 좌측 → 우측으로 이동하는 간격 : 1 또는 2로 설정   
* 패딩(Padding)
  * 특성맵의 크기가 입력 이미지의 크기보다 작아지지 않도록 하는 설정값
  * zero-padding : 입력 이미지의 가장자리에 0으로 패딩 값을 채운 후 특성맵을 추출한다.
* 풀링(Pooling)
  * 추출된 특성맵을 다운 샘플링하여 이미지 크기를 줄이는 레이어
  * 이미지 축소를 통해 연산량을 감소시킴, 과대적합(over-fitting)을 방지한다는 장점이 있다.
  * 최대 풀링(max pooling) : 해당 픽셀들 중 최대값이 출력값으로 선택되는 방법 → 합성곱 신경망(CNN)과 주로 사용됨
  * 평균 풀링(min pooling) : 해당 픽셀들의 평균값이 출력값으로 선택되는 방법

## 4.2 mnist를 이용한 간단한 CNN 구현
### 4.2.1 데이터 전처리
mnist 데이터를 불러와서 정규화를 해주고, CNN 모델에 집어넣기 위해 색상을 나타내는 채널을 1개 추가해주도록 한다.
mnist 데이터셋은 모노 컬러 이미지로, 1개의 채널을 이용해 색상을 나타낸다. 원하는 위치에 `tf.newaxis`를 작성해주면 된다.

```python
...
print(x_train.shape, x_valid.shape)

x_train_in = x_train[..., tf.newaxis]
x_valid_in = x_valid[..., tf.newaxis]

print(x_train_in.shape, x_valid_in.shape)
...
```
### 4.2.2 Sequential API로 모델 생성 및 학습
간단한 모델을 만들어 학습시켜 보자.
Sequential API를 사용해 합성곱 레이어 1개, 풀링 레이어 1개를 적용한 모델을 만들 것이다.  
최종 분류기로 Dense 레이어를 이용해야 하므로, Flatten 레이어를 추가하여 1차원 배열로 펼쳐주는 선작업이 필요하다.  

![CNN 모델 구조](/assets/images/CNNmodel.png)

- 합성곱 : Conv2D 레이어를 사용하고, (3, 3) 크기의 32가지 종류의 커널을 적용한다.  
  활성화 함수 : ReLU  
  레이어 이름 : conv  

- 풀링 레이어 : (2, 2) 크기, max pooling 적용  
- 최종 분류기  
  노드 개수 10개
  활성화 함수 : softmax (다중 분류 문제)

```python
#Sequential API를 이용한 모델 생성
model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32,(3, 3), activation='relu',
                          input_shape=(28, 28, 1), name='conv'),
    tf.keras.layers.MaxPooling2D((2, 2), name='pool'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, activation='softmax'),
])
```

### 4.2.3 모델 구조 파악
```
model.summary()

Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv (Conv2D)                (None, 26, 26, 32)        320       
_________________________________________________________________
pool (MaxPooling2D)          (None, 13, 13, 32)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 5408)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 10)                54090     
=================================================================
Total params: 54,410
Trainable params: 54,410
Non-trainable params: 0
_________________________________________________________________
```
- 입력 텐서 : (28, 28, 1)  
- Conv2D를 지나면 : (26, 26, 32)  
 ∵ 합성곱 필터의 크기가 (3, 3)이므로 가로 세로 크기가 각 2픽셀씩 감소  
 ∵ 커널의 종류가 32개이므로 32 종류의 피처를 추출  
- MaxPooling2D를 지나면 : (13, 13, 32)  
∵ (2, 2) 크기의 풀링 레이어를 적용해서 이미지 크기가 절반으로 줄어든다  
- Flatten layer를 지나면 1차원 벡터로 변환되므로 13 * 13 * 32 = 5408개의 파라미터가 만들어짐


## 4.3 mnist를 이용한 복잡한 CNN 구현
### 4.3.1 데이터셋 준비
텐서플로우 케라스의 Functional API를 사용해 복잡한 구조의 모델을 정의할 수 있다.
앞서 mnist 데이터를 불러와 훈련 데이터셋, 검증 데이터셋으로 구분하여 저장한다.
해당 데이터에 숫자의 홀, 짝을 구분하는 배열을 새로 만들어 추가한다.

```python
y_train_odd = []
for y in y_train :
    if y % 2 == 0:
        y_train_odd.append(0)
    else:
        y_train_odd.append(1)
        
y_train_odd = np.array(y_train_odd)
y_train_odd.shape

y_valid_odd = []
for y in y_valid :
    if y % 2 == 0 :
        y_valid_odd.append(0)
    else:
        y_valid_odd.append(1)

y_valid_odd = np.array(y_valid_odd)
y_valid_odd.shape
```
### 4.3.2 Functional API로 다중 입력, 다중 출력 레이어 생성
입력 레이어 input → (1) Conv2D 레이어, (2) Flatten 레이어 두 개의 입력으로 사용된다. == 2개의 출력

서로 다른 2개의 출력은 다른 레이어의 입력으로 사용되고 결론적으로 Concatenate 레이어에서 합쳐져 Dense 레이어를 지나도록 설정한다.
```python
inputs=tf.keras.layers.Input(shape=(28,28,1))

conv=tf.keras.layers.Conv2D(32,(3,3), activation='relu')(inputs)
pool=tf.keras.layers.MaxPooling2D((2,2))(conv)
flat=tf.keras.layers.Flatten()(pool)

flat_inputs=tf.keras.layers.Flatten()(inputs)
concat=tf.keras.layers.Concatenate()([flat, flat_inputs])
outputs=tf.keras.layers.Dense(10, activation='softmax')(concat)

model=tf.keras.models.Model(inputs=inputs, outputs=outputs)
```

![다중 입력, 다중 출력 레이어](/assets/images/functional_cnn.jpg)

### 4.3.3 다중 출력 분류 모델
숫자를 맞추는 분류 / 홀, 짝을 맞추는 분류 두 가지를 동시에 풀어내는 모델을 구현한다.  
최종 분류기의 출력 레이어가 2개가 되어야 하는 것이 핵심이다.  
그림의 출력 레이어를 살펴보면 10개인 텐서와 1개인 텐서 총 두개가 존재하는 것을 알 수 있다.  

```python
inputs = tf.keras.layers.Input(shape=(28, 28, 1), name='inputs')
conv = tf.keras.layers.Conv2D(32,(3, 3), activation='relu', name='conv2d_layer')(inputs)
pool = tf.keras.layers.MaxPooling2D((2, 2), name='maxpool_layer')(conv)
flat = tf.keras.layers.Flatten(name='flatten_layer')(pool)

flat_inputs = tf.keras.layers.Flatten()(inputs)
concat = tf.keras.layers.Concatenate()([flat, flat_inputs])
digit_outputs = tf.keras.layers.Dense(10, activation='softmax', name='digit_dense')(concat)

odd_outputs = tf.keras.layers.Dense(1, activation='sigmoid', name='odd_dense')(flat_inputs)

model = tf.keras.models.Model(inputs=inputs, outputs=[digit_outputs, odd_outputs])
```

![다중 출력 분류 모델](/assets/images/multi_output_cnn.png)

출력 레이어에 출력 값이 2개 있기 때문에 적용할 손실함수와 가중치도 각각 지정해줘야 한다. 딕셔너리 형태로 지정해주면 된다.

모델의 성능을 평가하기 위해 evaluate() 메소드를 사용한다.  
숫자를 맞추는 문제 : 약 98%, 홀수를 판단하는 문제 : 약 90%의 정확도를 가진다.  
홀수 판단 문제의 정확도가 떨어지므로 레이어를 더 추가해서 예측 정확도를 높일 수 있을 것이다.


### 4.3.4 전이 학습(Transfer learning)
전이 학습이란?  
: 앞서 훈련한 모델의 일부를 가져와 그대로 사용, 최종 출력층만 새롭게 추가해 모델을 재구성하는 것.

그대로 가져올 부분 : 앞서 세운 모델의 'flatten layer'인 flatten 레이어까지 → 베이스 모델 객체 생성
모델의 재구성 구조 : 베이스 모델을 레이어로 추가 + Dense 레이어를 새롭게 추가(최종 분류기 역할)

![base_model](/assets/images/base_model.png)

![digit_model](/assets/images/digit_model.png)

---
모델 인스턴스의 trainable 속성 : 파라미터 값이 훈련되어 계속 업데이트 되는 것.  
trainable 속성을 false로 지정하면 파라미터 값이 업데이트 되지 않는다. → base_model_frozen 구현 가능해짐

**다양한 모델 구성 방법, 파라미터 조정 방법에 대해 이해하고 넘어가는 것이 중요하다.**

> 모든 이미지 출처 : <파이썬 딥러닝 텐서플로 : 텐서플로와 딥러닝, 최적의 입문서!>
