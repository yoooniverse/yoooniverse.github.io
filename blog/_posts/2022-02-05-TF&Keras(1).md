---
title : 텐서플로우, 케라스 개념 정리
toc : true
toc_sticky : true
---


## PART2 TENSORFLOW
**텐서플로우의 기능 : 자동 미분 (automatic differentiation)**  

(1) create dataset  
	first, create dataset with ‘generator’ ( tf.random 모듈 )  
    * 정규분포를 갖는 10개의 숫자 데이터 -> 변수 x에 저장  
    * 선형 함수식을 만족하는 값을 대입 -> 변수 y에 저장  

> (참고)  
> ‘딥러닝 모델 학습’ 한 줄 요약 :  
> 모델의 예측 값 & 실제 값의 차이를 최소화하는 모델의 계수, 상수항을 찾는 과정  
> 여기서 ‘차이’ : loss  

ex_MSE(평균 제곱 오차)를 계산하는 손실함수를 정의해보자
```python
Def cal_mse (X, Y, a, b):
	y_pred = a * X + b
	squared_error = (y_pred - Y) ** 2
	mean_squared_error = tf.reduce_mean(sqaure_error)
	
	return mean_squared_error
```


- - - -

< 어떻게 학습이 이루어지는가 >   
1. 모델의 계수와 상수항에 해당하는 변수 a, b를 생성한다 <— 초기값은 둘 다 0.0  
2. 몇 번 돌릴지 설정 : 200에포크. 
3. With 구문 안에서 일어나는 일
* cal_mse 함수로 계산한 결과는 mse 변수에 저장  
* Gradient 함수 사용하여 얻어진 미분값 —> grad 변수에 저장 (계수 a : g_a, 상수항 b : g_b)  
* 미분값에 학습률 0.05를 적용하여 곱함 —> 기존 계수, 상수항에서 차감
* 차감된 값을 다음 에포크의 입력값으로 사용
4. A, b 값을 계속 업데이트 함(mse를 낮추는 방향으로)
5. 결과 : mse는 0에 가까워지고 실제 값에 근사하는 것을 확인 가능

예제 코드 : 
```python
#tf.GradientTape로 자동 미분 과정을 기록한다.

a = tf.Variable(0,0)
b = tf.Variable(0,0)

EPOCHS = 200

for epoch in range(1, EPOCHS + 1):
	with tf.GradientTape() as tape :
		mse = cal_mse(X, Y, a, b)
	
	grad = tape.gradient(mse, {‘a’:a, ‘b’:b})
	d_a, d_b = grad[‘a’], grad[‘b’]

	a.assign_sub(d_a * 0.05)
	b.assign_sub(d_b * 0.05)
	
	if epoch % 20 == 0 :
		print(“EPOCH %d - MSE: %.4f —— a: %.2f —— b: %.2f” %(epoch, mse, a, b))
```

- - - -

## PART3 KERAS
* 텐서플로우2가 1과 가장 다른 점 : 케라스 내장  
* 케라스 api : 고수준 딥러닝 라이브러리  
* 케라스의 특장점 :  
gpu 연산 수행 기능을 직접 돌리지 않고 백엔드 엔진을 지정하여 사용한다는 것 (텐서플로우 위에 만들어진 케라스)  
* 케라스의 목적 : 딥러닝 라이브러리를 쉽고 간결한 코드로 실행할 수 있도록 하는 것.


**주요 용어**
* 하이퍼 파라미터(hyper-parameter)  
-ML 모델 훈련 시 사용자가 직접 설정해주는 설정값 (ex 학습 속도, 반복 훈련 횟수 등)  
-사용자가 별도 설정하지 않으면 기본값이 자동으로 적용된다  
-모델의 예측 성능을 높이는 데 관여하는 키포인트들 (a.k.a 하이퍼파라미터 튜닝)     

* 과소적합 & 과대적합  
배경지식 : ML 학습에 쓰이는 데이터는 두 종류  
	(1) 훈련용 train set : ML 모델이 학습할 데이터  
	(2) 예측용 test set : ML 모델이 예측해야 하는 대상 데이터, 정답 레이블이 없다  
		- 훈련 데이터에서 패턴 학습을 진행 -> 모델이 완성됨  
		- 반복 학습의 효과 : 사람이 못찾는 패턴을 발견 -> 예측 성능이 좋은 모델 생성 
	- 과소적합 : 모델이 충분히 학습되지 않아 예측 성능이 떨어진 상태
	- 과대적합 : 학습 데이터를 지나치게 학습해서 그 결과가 과하게 적합된 상태
 
    
🐦 **가장 베스트 결과를 만들려면**  
과소적합, 과대적합 문제를 최소화 & 정확도를 최대한 높이기  
훈련 데이터 잘 구성하기 ( 훈련 데이터 예측 데이터 분포 동일하게 맞추기, 데이터 분석 전처리로 노이즈 없애기, 검증 성능이 가장 좋은 구간을 기준으로 최종 모델 결정하기 )

* 에포크  
에포크 = 1회 훈련 루프 (반복 훈련 시 훈련 데이터셋이 모두 1번 모델 훈련에 사용되는 것)

* 손실함수 : 예측 값, 정답 값의 차이 / 오차

* 지도학습  
	(1) 이진 분류
    * 손실함수 : binary-crossentropy / 활성화 함수 : sigmoid  

	(2) 다중 분류  
    * 손실함수 : categorical_crossentropy (다중 분류 대상 클래스가 원핫 벡터인 경우 사용)  
    * 손실함수 : sparse_categorical_crossentropy (레이블 값이 다른 숫자로 구분 된 경우 사용)  
    * 활성화 함수 : softmax


* 경사하강법  
딥러닝 모델 훈련 시, 모델 내부 가중치에 대한 미분 값을 구하고 목적 함수 값이 낮아지는 방향으로 차감 —> 최소 함수 값을 갖게 하는 방법
	* 최적화 함수(옵티마이저)의 근간이 되는 알고리즘
