---
title : 모델 세부 설정
toc : true
toc_sticky : true
---

> *"part 3 케라스(Keras) 04 모델 세부 설정"에 대한 정리*

## 모델 세부 설정
이전 포스팅에서 분류 모델을 만들면서 데이터 전처리부터 시작해 모델의 훈련 과정, 예측 결과 분석, 모델 성능 평가까지의 단계를 직접 코드로 입력해보았다. 각 절차와 구현한 모델의 특성에 따라 다양한 함수를 적용할 수 있는 옵션이 있음을 확인할 수 있었다.

이번 포스팅에서는 다음 챕터인 ’04 모델 세부 설정’ 부분에 대한 정리를 다룬다. 앞서 다룬 예측 모델의 개념과 베이스는 공통적으로 가져오지만 모델의 설정값을 더 디테일하게 다뤄보고자 한다.

## 4.1 초기값 설정
레이어의 초기화 방법을 다르게 설정할 수 있다.  
앞에서 Dense 레이어를 이용해 모델을 만들었는데, Dense 레이어의 초기화 방법은 ‘Glorot Uniform’으로 디폴트 설정이 되어있었다.  
레이어의 초기화 방법을 변경하고 싶을 때에는 kernel_initializer 매개변수를 이용하면 된다.

예제 코드에서는 기본값인 Glorot Uniform 대신 HeNormal 방법을 적용해 Dense 레이어를 초기화 한다.
그전에 먼저, Dense 레이어의 초기화 방법 디폴트 값을 알아보자
```python
dense = tf.keras.layers.Dense(256, activation=‘relu’)
dense.get_config()[‘kernel_initializer’]
```

HeNormal 방법을 적용해 초기화하기
```python
# 문자열을 넣어 바꾸기 (방법 1)
dense = tf.keras.layers.Dense(256, kernel_initializer=‘he_normal’, activation=‘relu’)
print(dense.get_config()[‘kernel_initializer’]

 # 클래스 인스턴스를 생성하여 바꾸기 (방법 2)
he_normal = tf.keras.initializers.HeNormal()
dense =  tf.keras.layers.Dense(256, kernel_initializer=he_normal, activation=‘relu’)
print(dense.get_config()[’kernel_initializer’])
```

참고) 케라스에서 자주 사용되는 초기화 목록
* glorot_normal, glorot_uniform : 글로럿 초기화
* lecun_normal, lecun_uniform : Yann Lecun 초기화
* he_normal, he_uniform : He 초기화
* random_normal, random_uniform : 정규 분포, 연속균등 분포 초기화

## 4.2 규제(regulation)
텐서플로우 케라스 레이어는 규제를 적용하지 않는게 디폴트이다.  
그렇지만 모델의 과대적합 현상이 발생할 경우 이를 해소하기 위해 L1, L2 규제를 적용하기도 한다.  
레이어의 규제를 설정할 때는 kernel_regularizer 매개변수를 이용하자.

먼저 레이어의 규제 디폴트값을 확인해보자
```python
Dense = tf.keras.layers.Dense(256, activation=‘relu’)
dense.get_config()
```

규제 지정하기
```python
# l1 규제 적용
dense = tf.keras.laters.Dense(256, kernel_regularizer=’l1’, activation=‘relu’)
print(dense.get_config()[‘kernel_regularizer’])

# 클래스 인스턴스 적용, alpha = 0.1 변경
regularizer = tf.keras.regularizers.l1(l1=0.1)
Dense = tf.keras.layers.Dense(256, kernel_regularizer=regularizer, activation=‘relu’)
print(dense.get_config()[‘kernel_regularizer’])
```

## 4.3 드롭아웃(Dropout)
딥러닝 모델의 가장 큰 문제점으로 지적되는 과대적합 이슈를 해결하기 위해 제시된 아이디어가 바로 ‘드롭아웃’이다.  
과대적합은 딥러닝 모델이 wide-and-deep의 특성을 가질 때 발생하는 현상이다. 훈련에 주어진 샘플에 과도하게 적합하도록 학습된 경우를 의미한다.  
여기서 wide-and-deep 모델은 모델의 층의 넓고 깊어진 경우를 지칭하는 용어이다.  
과대적합이 발생한 모델은 훈련 데이터로는 좋은 예측 결과를 만들어낼 수 있지만, 새롭게 만난 데이터를 가지고는 좋은 예측을 하지 못한다.  
다른 말로 표현하자면, ‘훈련 데이터셋에 너무 적응한 나머지 검증 데이터셋이나 테스트 데이터셋에 대해 일반화된 성능을 갖지 못하게 되는’ 문제가 발생하는 것이다.

(이러한 문제를 개선하기 위해 제안된 드롭아웃 아이디어는 실무적으로 상당히 좋은 효과를 보인다고 알려져 있다.)

![Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from
overfitting”, JMLR 2014](/assets/images/dropout.png)

위 두 그림의 가장 큰 차이는 노드들의 연결이다.  
왼쪽은 모든 노드들이 연결되어 있는 반면 오른쪽은 일부 노드들만 레이어 사이에서 연결되어 있음이 확인 가능하다.
노드의 일부 신호를 임의로 삭제함으로써 모델이 과대적합되는 것을 막는다는 것이 드롭아웃 아이디어의 메인 개념인 셈이다.  
노드의 신호를 확률적으로 제외하여 모델을 학습시킬 때, 그 과정에서 이용되는 가중치 파라미터의 개수가 현저히 줄어든다.

**물론, 훈련 데이터를 제외한 데이터로 예측을 진행할 때는 모든 노드들을 활성화 시켜야 한다.**

```python
# dropout 25% 비율 적용 (== 25%의 노드가 삭제됨)
tf.keras.layers.Dropout(0.25)
```

## 4.4 배치 정규화(Batch Normalization)
배치 정규화 : 각 층에서 활성화 함수를 통과하기 전 미니 배치의 스케일을 정규화 하는 것.

효과 : 안정적인 훈련이 가능하고 성능 향상에 많은 도움이 된다.

다음 층으로 데이터가 전달되기 전에 스케일을 조정하는 방식으로 정규화가 이루어진다.<br><br>

배치 정규화 적용 ❌  
```python
# model A : Dense + ReLu
model_a = tf.keras.Sequential([
tf.keras.layers.Flatten(input_shape=(28, 28)),
tf.keras.layers.Dense(64, activation=‘relu’),
tf.keras.layers.Dense(32, activation=‘relu’),
tf.keras.layers.Dense(10, activation=‘softmax’),
])
model_a.summary()
```

배치 정규화 적용 ⭕️  
```python
# model B : Dense + ReLu
model_b = tf.keras.Sequential([
tf.keras.layers.Flatten(input_shape=(28,28)),
tf.keras.layers.Dense(64),
tf.keras.layers.BatchNormalization(), # 배치 정규화 적용
tf.keras.layers.Activation(‘relu’),

tf.keras.layers.Dense(32),
tf.keras.layers.BatchNormalization(), # 배치 정규화 적용
tf.keras.layers.Activation(‘relu’),

tf.keras.layers.Dense(10, Activation=‘softmax’),
])
model_b.summary()
```

## 4.5 활성화 함수(activation)
```python
model_c = tf.keras.Sequential([
tf.keras.layers.Flatten(input_shape=(28,28)),
tf.keras.layers.Dense(64),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.LeakyReLU(alpha=0.2),

tf.keras.layers.Dense(32),
tf.keras.layers.BatchNormalization(),
tf.keras.layers.LeakyReLU(alpha=0.2)
tf.keras.layers.Dense(10, activation=‘softmax’),
])

model_c.summary()
```

model_a, model_b, model_c의 훈련 후 수렴 속도를 확인해보자

A : Dense 레이어 + ReLU 활성화 함수  
B : Dense 레이어 + 배치 정규화  
C : Dense 레이어 + 배치 정규화 + LeakyReLU(0.2) 활성화 함수

```python
model_a.compile(optimizer=‘adam’, loss=‘sparse_categorical_crossentropy’, metrics=[‘accuracy’])
model_b.compile(optimizer=‘adam’, loss=‘sparse_categorical_crossentropy’, metrics=[‘accuracy’])
model_c.compile(optimizer=‘adam’, loss=‘sparse_categorical_crossentropy’, metrics=[‘accuracy’])

history_a = model_a.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
history_b = model_b.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
history_c = model_c.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10)
```

결과 시각화 코드
```python
import matplotlib.pyplot as plt
import numpy as np

plt.figure(figsize=12, 9))
plt.plot(np.arange(1, 11), history_a.history[‘val_loss’], color=‘navy’, linestyle=‘:’)
plt.plot(np.arange(1, 11), history_a.history[‘val_loss’], color=‘tomato’, linestyle=‘-.’)
plt.plot(np.arange(1, 11), history_a.history[‘val_loss’], color=‘green’, linestyle=‘-’)

plt.title(‘Losses’, fontsize=20)
plt.xlabel(‘epochs’)
plt.ylabel(‘Losses’)
plt.legend([‘ReLU’, ‘BatchNorm + ReLU’, ‘batchnorm + LeakyReLU], fontsize=12)
plt.show()
```

