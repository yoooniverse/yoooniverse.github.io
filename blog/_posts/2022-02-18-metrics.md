---
title : "[Object Detection] Metric, 평가지표에 대하여"
toc : true
toc_sticky : true

categories:
  - Object detection
tags:
  - metrics
  - IoU
  - mAP
  - 
header: 
  teaser: "/assets/images/teaser.png"
---


### Metric : 평가지표
모델 생성 및 훈련시, 정답 ground truth 데이터와 비교했을 때 예측 값의 성능을 비교

## Metric의 종류
### 1. IoU

![IoU](/assets/images/iou_equation.png)

IoU = Area of Overlap / Area of Union  
2개의 바운딩 박스가 일치하는 정도를 측정하여 0.0~1.0 사이의 값으로 표현  
- Area of Overlap : 교집합 영역
- Area of Union : 전체 합집합 영역

단점 :  1개의 바운딩 박스와 1개의 바운딩 박스를 일일히 비교하는 방식  
실제 이미지 1장에는 여러 개의 바운딩 박스가 존재할 가능성이 있다.

여러 바운딩 박스들을 전체적으로 비교해야 하는데 IoU는 그렇게 하지 못한다는 단점이 존재한다.


> 참고)  
> IoU의 계산 결과 값이  
> - 0.5 이상이면 : 제대로 검출 되었다고 판단 (True Positive)  
> - 0.5 미만이면 : 잘못 검출되었다고 판단 (False Positive)
> 
> 이 기준이 되는 임계값(threshold)은 원하는 값으로 설정이 가능하다.



### 2. mAP
2.1 precision, recall, F1의 개념  

**precision** : (정밀도) 모델이 검출한 것이 얼마나 정확한가.  
(How accurate is your predictions)  

**recall** : (재현율) positive를 얼마나 잘 찾아내는가. (물체 검출을 놓치지 않고 얼마나 잘 찾는지)  
(How good you find all the positives) 실제 TRUE 인 것들 중 모델이 TRUE로 검출한 비율


![Precision, Recall formula](/assets/images/precision&recall.png)

예시로 precision과 recall을 각각 이해해보기  
예시 1) 어떤 모델이 사람 10명을 검출했다. 검출된 결과물 중 3명만 옳게 검출된 것이면(TP)  
precision 값은 3 / 10 = 0.3

예시 2) 어떤 모델이 사물 20개를 옳게 검출했어야 했다. 그런데 실제 옳다고 검출된 것은(TP) 10개다.  
recall 값은 10 / 20 = 0.5 

물체 검출 알고리즘의 성능을 평가할 때는 precision과 recall을 모두 고려해야 한다.
precision과 recall의 관계를 알아야 할 필요성이 있는 것이다.

이해가 쉽게 예를 들어 설명해보자.  
어떤 이미지에서 검출해야 하는 물체가 20개라고 가정하자. 그리고 내가 적용한 모델은 10개만 검출해냈다. 그리고 그 10개 중 6개만 맞게 검출되었다.  
이 때 precision과 recall 값을 각각 계산해보면,  
- precision  = 6 / 10 = 0.6  
- recall = 6 / 20 = 0.3  

각각 다른 성능값을 보이게 되는 것이다.

precision과 recall은 반비례한다는 특성을 가진다.  
정확도 or 정밀도가 높으면 검출율 or 재현율은 낮아지고  
정확도 or 정밀도가 높으면 검출율 or 재현율은 높아진다.  

**결론적으로, 모델의 성능을 판단하려면 precision과 recall 두 지표를 종합적으로 살펴봐야 한다는 것이다. → Average Precision(AP)**

**F1** : precision과 recall의 조화 평균.

![F1 formula](/assets/images/f1_score.png)

F1 값이 높을 수록 성능이 좋은 모델이라고 평가할 수 있는 것이다.


2.2 Average Precision(AP)  

Average Precision : recall 별 precision의 평균  
(confidence가 높은 예측결과 순으로 정렬되었을 때 몇번째 이미지까지를 비교대상으로 삼을 것인가?)

예시를 들어 살펴보자  
(인프런 강의에서 [여기](https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173) 자료를 참고하였으므로 이 글에서도 해당 자료를 가지고 정리하려고 한다)

![dataset](/assets/images/ap_example.png)

5개의 사과가 있는 데이터셋을 살펴보자. 해당 데이터셋에서 이루어진 예측들을 표로 정리한게 위의 표 그림이다.  
표의 배열 순서는 confidence가 높은 예측 순서대로 나열된 것이다.  
두번째 열은 각 예측이 옳은 예측인지, 틀린 예측인지 여부를 알려준다.  
해당 그래프를 작성하는 과정에서 IoU 임계값은 0.5 기준으로 설정되었다.


예를 들어 rank 3번의 precision, recall 값을 알아보자.  

Precision : 지금까지 3번의 검출을 했고, 그 중 2개만 옳게 검출했으므로 2 / 3 = 0.67  

Recall : 찾아야 하는 5개의 사과 중 지금까지 2개만 찾았으므로 2 / 5 = 0.4  

표를 보면 알 수 있듯이, recall 값은 밑으로 내려갈수록 일정하게 커진다.  
하지만 precision 값은 들쑥날쑥하다. false positive가 나오면 값이 낮아지고, true positive가 나오면 값이 올라간다.


이 값들을 그래프로 그려보면 숫자의 변화 양상을 더 쉽게 파악할 수 있다.

![precision-recall curve](/assets/images/precision_recall_curve.png)

앞서 말했듯이, recall 값이 커질 수록 precision 값이 낮아지는 반비례 형태의 그래프를 볼 수 있다.

Average Precision의 일반적 정의는 Precision-Recall 그래프의 아래 면적이다.
precision과 recall 값은 항상 0과 1 사이에 있다. AP값 역시 0과 1 사이에 위치한다.

mean Average Precision(mAP)은 AP에서 한발짝 더 나아가 class들의 AP 값의 평균을 구한 것이다.

> (참고한 사이트의 글은 파스칼, 코코 등 여러 데이터셋에 대해 이야기하고 있지만… 해당 포스트는 여기서 마무리 하고 보다 더 구체적인 이야기들은 다른 포스팅에서 다루려고 한다)
