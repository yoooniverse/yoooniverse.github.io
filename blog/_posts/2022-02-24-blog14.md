---
title : "[TensorFlow] 객체 탐지(Object Detection)"
excerpt : "[part 04 합성곱 신경망(CNN) 06 객체 탐지]에 대한 내용 정리"

toc : true
toc_sticky : true

categories :
  - Tensorflow

tags :
  - tensorflow


header :
  teaser : "/assets/images/teaser4.png"
---

> "part 04 합성곱 신경망(CNN) 06 객체 탐지"에 대한 내용 정리  
> "교재에서 제공하는 코드는 [여기](https://github.com/yoooniverse/tf-practice)에 전체 받아적어 두었다.

머신 러닝 분야 중 하나인 객체 탐지, object detection이라는 표현으로 더 잘 알려져 있다. 해당 분야에서 이미지를 인식하는 컴퓨터 비전 AI 기술이 가장 많이 응용된다. 자율주행 차량에서 도로 상황을 파악하거나, 장매물 인식 작업을 수행할 때 이용되는 기술이다.

object detection 문제는 두 가지 작업을 처리한다. 첫번째, 입력 이미지에서 여러 객체들을 찾아내고, 두번째, 각 객체가 무엇을 나타내는지 분류한다.  
- 이미지에서 객체를 찾아 그 객체를 둘러싸는 바운딩박스를 그리는데, 이를 위해 필요한 좌표값을 구할 때 '회귀 문제'를 이용한다.

### 1. 텐서플로우 허브
텐서플로우 허브(TensorFlow Hub) : 딥러닝 문제에 활용할 수 있는 사전학습 모델을 제공하는 저장소 [링크](https://www.tensorflow.org/hub)  
허브에서 제공하는 모델들을 그대로 가져와 분류문제에 사용하는 것, 제공된 모델을 베이스 모델로 삼아 개인 목적에 따라 커스텀 하는 것도 가능하다.  

#### 1.1 샘플 이미지 준비
[코드](https://github.com/yoooniverse/tf-practice/blob/main/4.5_object_detection_hub.ipynb)

이미 학습된 딥러닝 모델을 사용하는 것이기 때문에 별도의 모델 훈련 과정은 필요하지 않다. 모델을 활용해 객체 탐지 후, 샘플 이미지를 검출한다.  

사전 학습 모델은 배치 크기를 포함해 4차원 텐서를 입력받기 때문에 불러온 이미지에 축을 하나 추가해 4차원으로 만들어준다.

#### 1.2 사전 학습 모델
텐서플로우 허브는 Open Images v4 데이터셋으로 2가지 모델을 제공한다.

![Tensorflow hub](/assets/images/tensorflowhub.png)

책에서 사용하는 모델은 inception_resnet_v2모델을 사용한다. Faster R-CNN 알고리즘으로 구현되었다. 다른 모델(mobilenet_v2)보다 정확도가 높다는 특징이 있다.

> Faster R-CNN 알고리즘 : softmax 함수로 객체 분류, 바운딩 박스를 회귀로 찾는 R-CNN을 보완하는 object detection 알고리즘

load 함수로 모델을 불러온다.
```python
#사용할 모델의 링크를 가져와 load 함수에 넣어 모델을 불러온다.

model = tfhub.load("https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1")
```
모델의 시그니처 키를 활용해 모델 인스턴스를 생성한다.
```python
#모델 시그니처 확인
model.signatures.keys()

#시그니처 키를 이용해 object detection 모델 인스턴스를 만든다.
obj_detector = model.signatures['default']
obj_detector
```

#### 1.3 추론
생성한 모델 인스턴스에 앞서 전처리해둔 이미지를 입력한다. 모델은 inference 과정을 거쳐서 예측값을 반환한다. result 변수의 딕셔너리 키 배열을 확인하면 확인 가능한 값을 알 수 있다. 해당 목록에 나와있는 값들 중, <detection boxes, detection class entities, detection score> 세 가지를 사용할 것이다.
- detection boxes : 바운딩 박스 좌표
- detection class entities : 검출 클래스 아이디
- detection score : 검출 스코어

검출 스코어가 0.1보다 큰 경우만 바운딩 박스와 예측 클래스를 시각화한다. 최대 10개 객체만 표시되도록 설정한다.

### 2. YOLO 객체 탐지
YOLO 객체 탐지 모델 : 바운딩 박스와 예측 클래스를 다른 문제로 보지 않고 하나의 회귀 문제로 접근하는 개념이다. 빠른 처리 속도가 장점이다. (Darknet 제공)

![YOLO](/assets/images/yolo.png)

책에 의하면, YOLO 모델을 개인 로컬 PC에서 훈련시키는 것은 GPU와 훈련 시간 때문에 사실상 불가능하다고 한다. 따라서 개인이 YOLO 모델로 작업하기 위해서는 Darknet에서 제공하는 사전학습 모델을 활용하는 것이 가장 좋은 방법이다.

#### 2.1 Darknet YOLO 모델 추론하기
```python
!git clone https://github.com/AlexeyAB/darknet
# 객체 탐지에 사용할 이미지를 구글 코랩 파일에 업로드 한다.

#GPU를 사용할 수 있도록 Darknet의 Makefile을 수정한다
%cd darknet
!sed -i 's/GPU=0/GPU=1/' Makefile
!sed -i 's/CUDNN=0/CUDNN=1/' Makefile
!sed -i 's/CUDNN_HALF=0/CUDNN_HALF=1/' Makefile

#darknet 생성
!make

#모델 가중치를 가져온다
!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights
```

```python
import matplotlib.pylab as plt
import tensorflow as tf

#업로드한 샘플 이미지를 확인한다
plt.figure(figsize=(15,10))

img = tf.io.read_file('/content/gangnam.jpg')
img = tf.image.decode_jpeg(img, channels=3) #string -> unit8(숫자 텐서)로 변환
img = tf.image.convert_image_dtype(img, tf.float32) #0~1 범위로 정규화

plt.imshow(img)
```

```python
# Darknet 실행
# 샘플 이미지 코랩 파일 경로를 마지막에 추가한다.
!./darknet detector test cfg/coco.data cfg/yolov4.cfg yolov4.weights /content/gangnam.jpg

# 탐지된 결과를 별도 파일로 지정하여 저장한다.
plt.figure(figsize=(15,10))
img=tf.io.read_file('/content/darknet/predictions.jpg')
img=tf.image.decode_jpeg(img, channels=3)
img=tf.image.convert_image_dtype(img, tf.float32)
plt.imshow(img)
```
![결과 이미지](/assets/images/yolo_output.png)

#### 2.2 나만의 YOLO 모델 생성
YOLO 모델을 더 쉽게 이해하기 위해 간단한 형태의 YOLO 모델을 구현해본다.
앞서 올려둔 YOLO 모델 구조 사진을 보면, 해당 모델은 이미지를 총 49개의 셀로 나누어 객체를 탐지한다. 우리는 간단한 형태를 구현하므로, 셀의 개수를 대폭 줄여 **가로, 세로 3개의 셀**로 나눈다. 한 셀 당 1개의 박스를 그리고, **객체 탐지에 사용할 class의 수도 3개**로 줄인다.

필요한 패키지를 임포트하고,
```python
import tensorflow as tf
import numpy as np
import cv2 #이미지, 영상 처리에 사용하는 오픈소스 라이브러리
from google.colab.patches import cv2_imshow
```

파라미터를 설정한다.
```python
#이미지 크기
width_size = 256
hight_size = 256
channel_size = 3
img_size = (width_size,hight_size,channel_size)

cell_num = 3 #이미지를 나누는 크기
class_num = 3 #찾고자 하는 객체의 수
anchor_num = 1 #한 셀에 그릴 박스의 수
label_num = anchor_num * (5+class_num)

epoch_num=20000 #학습 수

#loss 비중
loss_p_rate=1.0
loss_cod_rate=5.0
loss_c_rate=1.0
loss_p_no_rate=0.5
```

책의 자료실에서 제공하는 [이미지 파일 3개](https://github.com/lovedlim/tensorflow/tree/main/Part%204)를 사용한다. 코랩 폴더에 해당 파일들을 업로드한다.

cv2라이브러리를 이용해 3개 도형을 랜덤한 위치에 그린다.
```python
# 랜덤하게 도형을 그리고, 실제 정답 값을 생성하는 함수 정의
# 0.png / 1.png / 2.png 파일이 필요함

def make_img_label():
    img = np.zeros((hight_size+400,width_size+400,channel_size)) # 0으로 이루어지고, 주어진 크기의 ndarray 배열 객체를 반환
    label = np.zeros((cell_num,cell_num,label_num))
    num_shape = np.random.randint(1,4)

    # numpy.random.choice(a, size=None, replace(복원, 비복원)=True, p(확률)=None)
    i = np.random.choice(range(cell_num),num_shape,replace=False)
    j = np.random.choice(range(cell_num),num_shape,replace=False)
    
    img_0 = cv2.imread('0.png')
    img_1 = cv2.imread('1.png')
    img_2 = cv2.imread('2.png')
    
    for n_h in range(num_shape):
        row = i[n_h]
        col = j[n_h]
        
        shape_type = np.random.randint(0,class_num)
        x_rate = np.random.rand()
        y_rate = np.random.rand()
        w_rate = np.random.rand() * 0.3 +0.1
        h_rate = np.random.rand() * 0.3 +0.1
                
        label[row,col]=[1,x_rate,y_rate,w_rate,h_rate,0,0,0]
        label[row,col,5+shape_type]=1

        x = int(x_rate * width_size/cell_num + col * width_size/cell_num)
        y = int(y_rate * hight_size/cell_num + row * hight_size/cell_num)
        w = int(w_rate * width_size/2) * 2
        h = int(h_rate * hight_size/2) * 2

        if(shape_type==0):
            input_img = cv2.resize(img_0,(w,h))
        if(shape_type==1):
            input_img = cv2.resize(img_1,(w,h))
        if(shape_type==2):
            input_img = cv2.resize(img_2,(w,h))

        img[y-int(h/2)+200 : y+int(h/2)+200, x-int(w/2)+200 : x+int(w/2)+200] =input_img
    img = img[200:200+hight_size,200:200+width_size]        
    
    return img,label

img,label = make_img_label()
cv2_imshow(img)
```

앞에서 생성한 이미지와 클래스를 입력하면, 탐지한 이미지에 박스를 그려주는 함수를 정의한다. 함수를 실행하명 경계 박스를 찾아서 표시한다.
```python
# 이미지와 정답(혹은 예측값)을 넣으면 박스를 그려주는 함수 정의
# 임계값 th 설정 (객체가 있다는 확률이 th이상일 때만 박스 생성)
def show_box(img,label,th=0.3):
    b_img = np.zeros((hight_size+400,width_size+400,3))
    b_img[200:200+hight_size,200:200+width_size] = img
    for i in range(cell_num):
        for j in range(cell_num):
            if(label[i,j,0] > th):
                x_rate = label[i,j,1]
                y_rate = label[i,j,2]
                w_rate = label[i,j,3]
                h_rate = label[i,j,4]
                shape_type=np.argmax(label[i,j,5:])
                if(shape_type==0):
                    line_color = [0,0,255]
                if(shape_type==1):
                    line_color = [255,0,0]
                if(shape_type==2):
                    line_color = [0,255,0]
                x = int(x_rate * width_size/3 + j * width_size/3)
                y = int(y_rate * hight_size/3 + i * hight_size/3)
                w = int(w_rate * width_size/2) * 2 + 20
                h = int(h_rate * hight_size/2) * 2 + 20

                cv2.rectangle(b_img,(x-int(w/2)+200,y-int(h/2)+200),(x+int(w/2)+200,y+int(h/2)+200),line_color)
                
    b_img = b_img[200:200+hight_size,200:200+width_size]

    return b_img

cv2_imshow(show_box(img,label))
```

전이 학습 방법을 사용하 이미지 특징 추출에 좋은 성능을 보이는 모델을 활용하는 것도 좋은 방법이다.
베이스 모델 : VGG16
마지막 객체 탐지 분류기 : Conv2D + Dense 레이어
```python
# VGG16모델을 베이스로 마지막 부분만 수정하는 모델 생성 (전이 학습)
vgg_model = tf.keras.applications.VGG16(include_top=False,input_shape=img_size)
vgg_model.trainable=False

i=tf.keras.Input(shape=img_size)
out=tf.keras.layers.Lambda((lambda x : x/255.))(i)
out = vgg_model(out)
out = tf.keras.layers.Conv2D(256,3,padding='same')(out)
out = tf.keras.layers.Conv2D(128,3,padding='same')(out)
out = tf.keras.layers.Conv2D(64,3,padding='same')(out)
out = tf.keras.layers.Flatten()(out)
out = tf.keras.layers.Dense(1024,activation='relu')(out)
out = tf.keras.layers.Dense(3*3*8,activation='sigmoid')(out)
out = tf.keras.layers.Reshape((3,3,8))(out)

yolo_model = tf.keras.Model(inputs=[i],outputs=[out])
opt = tf.keras.optimizers.Adam(0.00001)

# 모델 요약
yolo_model.summary()
```

이미지를 총 9개의 셀로 나누고 셀마다 학습을 진행한다.  
객체가 있는 셀의 경우 : 확률 / 박스 위치 및 크기 / 클래스 종류 세 가지를 모두 학습한다.
객체가 없는 셀의 경우 : 객체가 없는 확률만 학습한다.
```python
# 학습과정을 동영상으로 기록
fcc=cv2.VideoWriter_fourcc(*'DIVX')
out=cv2.VideoWriter('hjk_yolo.avi',fcc,1.0,(width_size,hight_size))


for e in range(epoch_num):
    img,label = make_img_label()
    img = np.reshape(img,(1,hight_size,width_size,3))
    label = np.reshape(label,(1,3,3,8))
    loss_p_list=[]
    loss_cod_list = []
    loss_c_list = []
    loss_p_no_list = []
    with tf.GradientTape() as tape:
        pred = yolo_model(img)
        # 이미지를 구분한 셀을 탐험
        for i in range(3):
            for j in range(3):
                # 해당 셀에 객체가 있을 경우는 확률, 박스 크기, 클래스까지 모두 Loss로 계산
                if(label[0,i,j,0]==1):
                    loss_p_list.append(tf.square(label[0,i,j,0]-pred[0,i,j,0]))
                    loss_cod_list.append(tf.square(label[0,i,j,1]-pred[0,i,j,1]))
                    loss_cod_list.append(tf.square(label[0,i,j,2]-pred[0,i,j,2]))
                    loss_cod_list.append(tf.square(label[0,i,j,3]-pred[0,i,j,3]))
                    loss_cod_list.append(tf.square(label[0,i,j,4]-pred[0,i,j,4]))
                    loss_c_list.append(tf.square(label[0,i,j,5]-pred[0,i,j,5]))
                    loss_c_list.append(tf.square(label[0,i,j,6]-pred[0,i,j,6]))
                    loss_c_list.append(tf.square(label[0,i,j,7]-pred[0,i,j,7]))
                # 해당 셀에 객체가 없을 경우 객체가 없을 확률만 Loss로 계산
                else:
                    loss_p_no_list.append(tf.square(label[0,i,j,0]-pred[0,i,j,0]))
        loss_p=tf.reduce_mean(loss_p_list)
        loss_cod =tf.reduce_mean(loss_cod_list)
        loss_c = tf.reduce_mean(loss_c_list)
        loss_p_no = tf.reduce_mean(loss_p_no_list)
        # 각 Loss를 비중을 곱해 더해 최종 Loss를 계산
        loss = loss_p_rate * loss_p + loss_cod_rate * loss_cod + loss_c_rate * loss_c + loss_p_no_rate * loss_p_no
    # Loss에 대한 Grad를 구하고, 각 파라미터를 업데이트
    vars = yolo_model.trainable_variables
    grad = tape.gradient(loss, vars)
    opt.apply_gradients(zip(grad, vars))
    # 100번 마다 동영상에 이미지를 기록한다
    if(e%100==0):
        img = np.reshape(img,(256,256,3))
        label = pred.numpy()
        label = np.reshape(label,(3,3,8))
        sample_img = np.uint8(show_box(img,label))
        out.write(sample_img)
    print(e,"완료",loss.numpy())    
out.release()
```

<video width="100%" height="100%" controls="controls">
  <source src="/assets/images/hjk_yolo.mp4" type="video/mp4">
</video>



<iframe id="video" width="100%" height="100%" src="/assets/images/hjk_yolo.mp4" frameborder="0">
</iframe>